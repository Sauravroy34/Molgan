{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03e8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_model = pickle.load(gzip.open('data/NP_score.pkl.gz'))\n",
    "SA_model = {i[j]: float(i[0]) for i in pickle.load(gzip.open('data/SA_score.pkl.gz')) for j in range(1, len(i))}\n",
    "\n",
    "\n",
    "class MolecularMetrics(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def _avoid_sanitization_error(op):\n",
    "        try:\n",
    "            return op()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def remap(x, x_min, x_max):\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_lambda(x):\n",
    "        return x is not None and Chem.MolToSmiles(x) != ''\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_lambda_special(x):\n",
    "        s = Chem.MolToSmiles(x) if x is not None else ''\n",
    "        return x is not None and '*' not in s and '.' not in s and s != ''\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_scores(mols):\n",
    "        return np.array(list(map(MolecularMetrics.valid_lambda_special, mols)), dtype=np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_filter(mols):\n",
    "        return list(filter(MolecularMetrics.valid_lambda, mols))\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_total_score(mols):\n",
    "        return np.array(list(map(MolecularMetrics.valid_lambda, mols)), dtype=np.float32).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_scores(mols, data):\n",
    "        return np.array(\n",
    "            list(map(lambda x: MolecularMetrics.valid_lambda(x) and Chem.MolToSmiles(x) not in data.smiles, mols)))\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_filter(mols, data):\n",
    "        return list(filter(lambda x: MolecularMetrics.valid_lambda(x) and Chem.MolToSmiles(x) not in data.smiles, mols))\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_total_score(mols, data):\n",
    "        return MolecularMetrics.novel_scores(MolecularMetrics.valid_filter(mols), data).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_scores(mols):\n",
    "        smiles = list(map(lambda x: Chem.MolToSmiles(x) if MolecularMetrics.valid_lambda(x) else '', mols))\n",
    "        return np.clip(\n",
    "            0.75 + np.array(list(map(lambda x: 1 / smiles.count(x) if x != '' else 0, smiles)), dtype=np.float32), 0, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_total_score(mols):\n",
    "        v = MolecularMetrics.valid_filter(mols)\n",
    "        s = set(map(lambda x: Chem.MolToSmiles(x), v))\n",
    "        return 0 if len(v) == 0 else len(s) / len(v)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def novel_and_unique_total_score(mols, data):\n",
    "    #     return ((MolecularMetrics.unique_scores(mols) == 1).astype(float) * MolecularMetrics.novel_scores(mols,\n",
    "    #                                                                                                       data)).sum()\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def reconstruction_scores(data, model, session, sample=False):\n",
    "    #\n",
    "    #     m0, _, _, a, x, _, f, _, _ = data.next_validation_batch()\n",
    "    #     feed_dict = {model.edges_labels: a, model.nodes_labels: x, model.node_features: f, model.training: False}\n",
    "    #\n",
    "    #     try:\n",
    "    #         feed_dict.update({model.variational: False})\n",
    "    #     except AttributeError:\n",
    "    #         pass\n",
    "    #\n",
    "    #     n, e = session.run([model.nodes_gumbel_argmax, model.edges_gumbel_argmax] if sample else [\n",
    "    #         model.nodes_argmax, model.edges_argmax], feed_dict=feed_dict)\n",
    "    #\n",
    "    #     n, e = np.argmax(n, axis=-1), np.argmax(e, axis=-1)\n",
    "    #\n",
    "    #     m1 = [data.matrices2mol(n_, e_, strict=True) for n_, e_ in zip(n, e)]\n",
    "    #\n",
    "    #     return np.mean([float(Chem.MolToSmiles(m0_) == Chem.MolToSmiles(m1_)) if m1_ is not None else 0\n",
    "    #             for m0_, m1_ in zip(m0, m1)])\n",
    "\n",
    "    @staticmethod\n",
    "    def natural_product_scores(mols, norm=False):\n",
    "\n",
    "        # calculating the score\n",
    "        scores = [sum(NP_model.get(bit, 0)\n",
    "                      for bit in Chem.rdMolDescriptors.GetMorganFingerprint(mol,\n",
    "                                                                            2).GetNonzeroElements()) / float(\n",
    "            mol.GetNumAtoms()) if mol is not None else None\n",
    "                  for mol in mols]\n",
    "\n",
    "        # preventing score explosion for exotic molecules\n",
    "        scores = list(map(lambda score: score if score is None else (\n",
    "            4 + math.log10(score - 4 + 1) if score > 4 else (\n",
    "                -4 - math.log10(-4 - score + 1) if score < -4 else score)), scores))\n",
    "\n",
    "        scores = np.array(list(map(lambda x: -4 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, -3, 1), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def quantitative_estimation_druglikeness_scores(mols, norm=False):\n",
    "        return np.array(list(map(lambda x: 0 if x is None else x, [\n",
    "            MolecularMetrics._avoid_sanitization_error(lambda: QED.qed(mol)) if mol is not None else None for mol in\n",
    "            mols])))\n",
    "\n",
    "    @staticmethod\n",
    "    def water_octanol_partition_coefficient_scores(mols, norm=False):\n",
    "        scores = [MolecularMetrics._avoid_sanitization_error(lambda: Crippen.MolLogP(mol)) if mol is not None else None\n",
    "                  for mol in mols]\n",
    "        scores = np.array(list(map(lambda x: -3 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, -2.12178879609, 6.0429063424), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_SAS(mol):\n",
    "        fp = Chem.rdMolDescriptors.GetMorganFingerprint(mol, 2)\n",
    "        fps = fp.GetNonzeroElements()\n",
    "        score1 = 0.\n",
    "        nf = 0\n",
    "        # for bitId, v in fps.items():\n",
    "        for bitId, v in fps.items():\n",
    "            nf += v\n",
    "            sfp = bitId\n",
    "            score1 += SA_model.get(sfp, -4) * v\n",
    "        score1 /= nf\n",
    "\n",
    "        # features score\n",
    "        nAtoms = mol.GetNumAtoms()\n",
    "        nChiralCenters = len(Chem.FindMolChiralCenters(\n",
    "            mol, includeUnassigned=True))\n",
    "        ri = mol.GetRingInfo()\n",
    "        nSpiro = Chem.rdMolDescriptors.CalcNumSpiroAtoms(mol)\n",
    "        nBridgeheads = Chem.rdMolDescriptors.CalcNumBridgeheadAtoms(mol)\n",
    "        nMacrocycles = 0\n",
    "        for x in ri.AtomRings():\n",
    "            if len(x) > 8:\n",
    "                nMacrocycles += 1\n",
    "\n",
    "        sizePenalty = nAtoms ** 1.005 - nAtoms\n",
    "        stereoPenalty = math.log10(nChiralCenters + 1)\n",
    "        spiroPenalty = math.log10(nSpiro + 1)\n",
    "        bridgePenalty = math.log10(nBridgeheads + 1)\n",
    "        macrocyclePenalty = 0.\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # This differs from the paper, which defines:\n",
    "        #  macrocyclePenalty = math.log10(nMacrocycles+1)\n",
    "        # This form generates better results when 2 or more macrocycles are present\n",
    "        if nMacrocycles > 0:\n",
    "            macrocyclePenalty = math.log10(2)\n",
    "\n",
    "        score2 = 0. - sizePenalty - stereoPenalty - \\\n",
    "                 spiroPenalty - bridgePenalty - macrocyclePenalty\n",
    "\n",
    "        # correction for the fingerprint density\n",
    "        # not in the original publication, added in version 1.1\n",
    "        # to make highly symmetrical molecules easier to synthetise\n",
    "        score3 = 0.\n",
    "        if nAtoms > len(fps):\n",
    "            score3 = math.log(float(nAtoms) / len(fps)) * .5\n",
    "\n",
    "        sascore = score1 + score2 + score3\n",
    "\n",
    "        # need to transform \"raw\" value into scale between 1 and 10\n",
    "        min = -4.0\n",
    "        max = 2.5\n",
    "        sascore = 11. - (sascore - min + 1) / (max - min) * 9.\n",
    "        # smooth the 10-end\n",
    "        if sascore > 8.:\n",
    "            sascore = 8. + math.log(sascore + 1. - 9.)\n",
    "        if sascore > 10.:\n",
    "            sascore = 10.0\n",
    "        elif sascore < 1.:\n",
    "            sascore = 1.0\n",
    "\n",
    "        return sascore\n",
    "\n",
    "    @staticmethod\n",
    "    def synthetic_accessibility_score_scores(mols, norm=False):\n",
    "        scores = [MolecularMetrics._compute_SAS(mol) if mol is not None else None for mol in mols]\n",
    "        scores = np.array(list(map(lambda x: 10 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, 5, 1.5), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def diversity_scores(mols, data):\n",
    "        rand_mols = np.random.choice(data.data, 100)\n",
    "        fps = [Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048) for mol in rand_mols]\n",
    "\n",
    "        scores = np.array(\n",
    "            list(map(lambda x: MolecularMetrics.__compute_diversity(x, fps) if x is not None else 0, mols)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, 0.9, 0.945), 0.0, 1.0)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_diversity(mol, fps):\n",
    "        ref_fps = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)\n",
    "        dist = DataStructs.BulkTanimotoSimilarity(ref_fps, fps, returnDistance=True)\n",
    "        score = np.mean(dist)\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def drugcandidate_scores(mols, data):\n",
    "\n",
    "        scores = (MolecularMetrics.constant_bump(\n",
    "            MolecularMetrics.water_octanol_partition_coefficient_scores(mols, norm=True), 0.210,\n",
    "            0.945) + MolecularMetrics.synthetic_accessibility_score_scores(mols,\n",
    "                                                                           norm=True) + MolecularMetrics.novel_scores(\n",
    "            mols, data) + (1 - MolecularMetrics.novel_scores(mols, data)) * 0.3) / 4\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def constant_bump(x, x_low, x_high, decay=0.025):\n",
    "        return np.select(condlist=[x <= x_low, x >= x_high],\n",
    "                         choicelist=[np.exp(- (x - x_low) ** 2 / decay),\n",
    "                                     np.exp(- (x - x_high) ** 2 / decay)],\n",
    "                         default=np.ones_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be878d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty( y, x):\n",
    "        weight = torch.ones(y.size())\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx ** 2, dim=1))\n",
    "        return torch.mean((dydx_l2norm - 1) ** 2)\n",
    "\n",
    "def label2onehot(labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "        out = torch.zeros(list(labels.size()) + [dim])\n",
    "        out.scatter_(len(out.size()) - 1, labels.unsqueeze(-1), 1.)\n",
    "        return out\n",
    "\n",
    "def sample_z(batch_size):\n",
    "        return np.random.normal(0, 1, size=(batch_size, 32))\n",
    "\n",
    "def postprocess(inputs, method = \"soft_gumbel\", temperature=1.):\n",
    "        def listify(x):\n",
    "            return x if type(x) == list or type(x) == tuple else [x]\n",
    "\n",
    "        def delistify(x):\n",
    "            return x if len(x) > 1 else x[0]\n",
    "\n",
    "        if method == 'soft_gumbel':\n",
    "            softmax = [F.gumbel_softmax(e_logits.contiguous().view(-1, e_logits.size(-1))\n",
    "                                        / temperature, hard=False).view(e_logits.size())\n",
    "                       for e_logits in listify(inputs)]\n",
    "        elif method == 'hard_gumbel':\n",
    "            softmax = [F.gumbel_softmax(e_logits.contiguous().view(-1, e_logits.size(-1))\n",
    "                                        / temperature, hard=True).view(e_logits.size())\n",
    "                       for e_logits in listify(inputs)]\n",
    "        else:\n",
    "            softmax = [F.softmax(e_logits / temperature, -1)\n",
    "                       for e_logits in listify(inputs)]\n",
    "\n",
    "        return [delistify(e) for e in (softmax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820fb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SparseMolecularDataset()\n",
    "data.load(\"qm9_5k.sparsedataset\")\n",
    "\n",
    "b_dim = data.bond_num_types\n",
    "m_dim = data.atom_num_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(mols,metric =\"validity,qed\"):\n",
    "        rr = 1.\n",
    "        for m in ('logp,sas,qed,unique' if metric == 'all' else metric).split(','):\n",
    "\n",
    "            if m == 'np':\n",
    "                rr *= MolecularMetrics.natural_product_scores(mols, norm=True)\n",
    "            elif m == 'logp':\n",
    "                rr *= MolecularMetrics.water_octanol_partition_coefficient_scores(mols, norm=True)\n",
    "            elif m == 'sas':\n",
    "                rr *= MolecularMetrics.synthetic_accessibility_score_scores(mols, norm=True)\n",
    "            elif m == 'qed':\n",
    "                rr *= MolecularMetrics.quantitative_estimation_druglikeness_scores(mols, norm=True)\n",
    "            elif m == 'novelty':\n",
    "                rr *= MolecularMetrics.novel_scores(mols, data)\n",
    "            elif m == 'dc':\n",
    "                rr *= MolecularMetrics.drugcandidate_scores(mols, data)\n",
    "            elif m == 'unique':\n",
    "                rr *= MolecularMetrics.unique_scores(mols)\n",
    "            elif m == 'diversity':\n",
    "                rr *= MolecularMetrics.diversity_scores(mols, data)\n",
    "            elif m == 'validity':\n",
    "                rr *= MolecularMetrics.valid_scores(mols)\n",
    "            else:\n",
    "                raise RuntimeError('{} is not defined as a metric'.format(m))\n",
    "\n",
    "        return rr.reshape(-1, 1)\n",
    "    \n",
    "def get_reward(n_hat, e_hat, method = \"soft_gumbel\"):\n",
    "        (edges_hard, nodes_hard) = postprocess((e_hat, n_hat), method)\n",
    "        edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "        mols = [data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "                for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "        rewards = torch.from_numpy(reward(mols))\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144dab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(y, x):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = torch.ones(y.size())\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)\n",
    "\n",
    "def get_gen_mols( n_hat, e_hat):\n",
    "        (edges_hard, nodes_hard) = postprocess((e_hat, n_hat))\n",
    "        edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "        mols = [data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "                for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "        return mols\n",
    "\n",
    "def all_scores(mols, data, norm=False, reconstruction=False):\n",
    "    m0 = {k: list(filter(lambda e: e is not None, v)) for k, v in {\n",
    "        'NP': MolecularMetrics.natural_product_scores(mols, norm=norm),\n",
    "        'QED': MolecularMetrics.quantitative_estimation_druglikeness_scores(mols),\n",
    "        'Solute': MolecularMetrics.water_octanol_partition_coefficient_scores(mols, norm=norm),\n",
    "        'SA': MolecularMetrics.synthetic_accessibility_score_scores(mols, norm=norm),\n",
    "        'diverse': MolecularMetrics.diversity_scores(mols, data),\n",
    "        'drugcand': MolecularMetrics.drugcandidate_scores(mols, data)}.items()}\n",
    "\n",
    "    m1 = {'valid': MolecularMetrics.valid_total_score(mols) * 100,\n",
    "          'unique': MolecularMetrics.unique_total_score(mols) * 100,\n",
    "          'novel': MolecularMetrics.novel_total_score(mols, data) * 100}\n",
    "\n",
    "    return m0, m1\n",
    "\n",
    "def random_string(string_len=3):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(string_len))\n",
    "\n",
    "def save_mol_img(mols, f_name='tmp.png', is_test=False):\n",
    "    orig_f_name = f_name\n",
    "    for a_mol in mols:\n",
    "        try:\n",
    "            if Chem.MolToSmiles(a_mol) is not None:\n",
    "                print('Generating molecule')\n",
    "\n",
    "                if is_test:\n",
    "                    f_name = orig_f_name\n",
    "                    f_split = f_name.split('.')\n",
    "                    f_split[-1] = random_string() + '.' + f_split[-1]\n",
    "                    f_name = ''.join(f_split)\n",
    "\n",
    "                rdkit.Chem.Draw.MolToFile(a_mol, f_name)\n",
    "                a_smi = Chem.MolToSmiles(a_mol)\n",
    "                mol_graph = read_smiles(a_smi)\n",
    "\n",
    "                break\n",
    "\n",
    "                # if not is_test:\n",
    "                #     break\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3b3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50886073",
   "metadata": {},
   "outputs": [],
   "source": [
    "G =  Generator()\n",
    "D = MolGANDiscriminator()\n",
    "R = MolGANDiscriminator()\n",
    "\n",
    "G_optim = torch.optim.Adam(G.parameters(),lr=1e-3)\n",
    "D_optim = torch.optim.Adam(D.parameters(),lr=1e-3)\n",
    "R_optim = torch.optim.Adam(R.parameters(),lr=1e-3)\n",
    "\n",
    "def reset_grad():\n",
    "    G_optim.zero_grad()\n",
    "    D_optim.zero_grad()\n",
    "    R_optim.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "la = 0\n",
    "la_gp = 10\n",
    "n_critic = 5\n",
    "g_lr = 1e-4\n",
    "d_lr = 1e-4\n",
    "num_steps = (len(data) // 32)\n",
    "def train(epoch_i, train_val_test='val',mode = \"train\"):\n",
    "    \n",
    "    \n",
    "    if epoch_i < 0:\n",
    "        cur_la = 0\n",
    "    else:\n",
    "        cur_la = la\n",
    "      \n",
    "    losses = defaultdict(list)\n",
    "    scores = defaultdict(list)  \n",
    "    the_step = num_steps\n",
    "    \n",
    "    if train_val_test == 'val':\n",
    "        if mode == 'train':\n",
    "                the_step = 1\n",
    "        print('[Validating]')\n",
    "        \n",
    "    for a_step in range(the_step):\n",
    "        if train_val_test == 'val':\n",
    "            mols, _, _, a, x, _, _, _, _ = data.next_validation_batch()\n",
    "            z = sample_z(a.shape[0])\n",
    "        elif train_val_test == 'train':\n",
    "            \n",
    "            mols, _, _, a, x, _, _, _, _ = data.next_train_batch(32)\n",
    "            z = sample_z(32)\n",
    "            \n",
    "    a= a.astype(np.int64)\n",
    "    x= x.astype(np.int64)\n",
    "    z = sample_z(32)\n",
    "    a = torch.from_numpy(a).long()         # Adjacency.\n",
    "    x = torch.from_numpy(x).long()        # Nodes.\n",
    "    a_tensor = label2onehot(labels =a, dim =b_dim)\n",
    "    x_tensor = label2onehot(x, m_dim)\n",
    "    z = torch.from_numpy(z).float()\n",
    "\n",
    "    cur_step = num_steps * epoch_i + a_step\n",
    "\n",
    "    logits_real, features_real = D(x_tensor,a_tensor)\n",
    "    edges_logits, nodes_logits = G(z)\n",
    "\n",
    "    (edges_hat, nodes_hat) = postprocess((edges_logits, nodes_logits))\n",
    "    logits_fake, features_fake = D(nodes_hat,edges_hat)\n",
    "\n",
    "    eps = torch.rand(logits_real.size(0), 1, 1, 1)\n",
    "    x_int0 = (eps * a_tensor + (1. - eps) * edges_hat).requires_grad_(True)\n",
    "    x_int1 = (eps.squeeze(-1) * x_tensor + (1. - eps.squeeze(-1)) * nodes_hat).requires_grad_(True)\n",
    "    \n",
    "    grad0, grad1 = D(x_int1, x_int0)\n",
    "    grad_penalty = gradient_penalty(grad0, x_int0) + gradient_penalty(grad1, x_int1)\n",
    "    \n",
    "    d_loss_real = torch.mean(logits_real)\n",
    "    d_loss_fake = torch.mean(logits_fake)\n",
    "    loss_D = -d_loss_real + d_loss_fake + la_gp * grad_penalty\n",
    "    \n",
    "    if cur_la > 0:\n",
    "            losses['l_D/R'].append(d_loss_real.item())\n",
    "            losses['l_D/F'].append(d_loss_fake.item())\n",
    "            losses['l_D'].append(loss_D.item())\n",
    "\n",
    "            # Optimise discriminator.\n",
    "    if train_val_test == 'train' and cur_step % n_critic != 0 and cur_la > 0:\n",
    "                reset_grad()\n",
    "                loss_D.backward()\n",
    "                G_optim.step()\n",
    "                \n",
    "    \n",
    "    value_logit_real, _ = R(x_tensor, a_tensor)\n",
    "    value_logit_fake, _ = R(nodes_hat,  edges_hat)\n",
    "    f_loss = (torch.mean(features_real, 0) - torch.mean(features_fake, 0)) ** 2\n",
    "\n",
    "    reward_r = torch.from_numpy(reward(mols))\n",
    "    reward_f = get_reward(nodes_hat, edges_hat)\n",
    "    print(reward_f)\n",
    "    loss_G = -logits_fake\n",
    "    \n",
    "    loss_V = torch.abs(value_logit_real - reward_r) + torch.abs(value_logit_fake - reward_f)\n",
    "    loss_RL = -value_logit_fake\n",
    "\n",
    "    loss_G = torch.mean(loss_G)\n",
    "    loss_V = torch.mean(loss_V)\n",
    "    loss_RL = torch.mean(loss_RL)\n",
    "    losses['l_G'].append(loss_G.item())\n",
    "    losses['l_RL'].append(loss_RL.item())\n",
    "    losses['l_V'].append(loss_V.item())\n",
    "\n",
    "    alpha = torch.abs(loss_G.detach() / loss_RL.detach()).detach()\n",
    "    train_step_G = cur_la * loss_G + (1 - cur_la) * alpha * loss_RL\n",
    "\n",
    "    train_step_V = loss_V\n",
    "    if train_val_test == 'train':\n",
    "            reset_grad()\n",
    "    \n",
    "    if cur_step % n_critic == 0:\n",
    "        train_step_V.backward()\n",
    "        R_optim.step()\n",
    "\n",
    "    if train_val_test == 'val':\n",
    "        mols = get_gen_mols(nodes_logits, edges_logits)\n",
    "        m0, m1 = all_scores(mols, data, norm=True)  # 'mols' is output of Fake Reward\n",
    "        for k, v in m1.items():\n",
    "                scores[k].append(v)\n",
    "        for k, v in m0.items():\n",
    "                scores[k].append(np.array(v)[np.nonzero(v)].mean())\n",
    "        \n",
    "        \n",
    "        mol_f_name = os.path.join(\"Home\", 'mol-{}.png'.format(epoch_i))\n",
    "        save_mol_img(mols, mol_f_name, is_test=mode == 'test')\n",
    "    \n",
    "        is_first = True\n",
    "        for tag, value in losses.items():\n",
    "                if is_first:\n",
    "                    log += \"\\n{}: {:.2f}\".format(tag, np.mean(value))\n",
    "                    is_first = False\n",
    "                else:\n",
    "                    log += \", {}: {:.2f}\".format(tag, np.mean(value))\n",
    "        is_first = True\n",
    "        for tag, value in scores.items():\n",
    "                if is_first:\n",
    "                    log += \"\\n{}: {:.2f}\".format(tag, np.mean(value))\n",
    "                    is_first = False\n",
    "                else:\n",
    "                    log += \", {}: {:.2f}\".format(tag, np.mean(value))\n",
    "        print(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurav/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_val_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs - > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch_i, train_val_test, mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m logits_fake, features_fake \u001b[38;5;241m=\u001b[39m D(nodes_hat,edges_hat)\n\u001b[1;32m     49\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(logits_real\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m x_int0 \u001b[38;5;241m=\u001b[39m (\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43medges_hat\u001b[49m)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m x_int1 \u001b[38;5;241m=\u001b[39m (eps\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m x_tensor \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m eps\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m nodes_hat)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m grad0, grad1 \u001b[38;5;241m=\u001b[39m D(x_int1, x_int0)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed2a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
