{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b03e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pickle\n",
    "import gzip\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import Crippen\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sparse_molecular_dataset import SparseMolecularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8517a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=32, N=9, T=5, Y=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim \n",
    "        self.N = N  \n",
    "        self.T = T  \n",
    "        self.Y = Y  \n",
    "        \n",
    "        self.atom_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, N * T),\n",
    "            nn.Softmax() \n",
    "        )\n",
    "        \n",
    "        self.adj_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, N * N * Y),\n",
    "            nn.Softmax()  \n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        x = self.atom_mlp(z) \n",
    "        x = x.view(batch_size, self.N, self.T)  # batch_size × N × T\n",
    "        \n",
    "        a = self.adj_mlp(z)  # batch_size × (N*N*Y)\n",
    "        a = a.view(batch_size, self.N, self.N, self.Y)  # batch_size × N × N × Y\n",
    "        \n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fb7687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalGraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_features, u, edge_type_num, dropout_rate=0.01 ,activation = nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        self.edge_type_num = edge_type_num\n",
    "        self.u = u\n",
    "        self.adj_list = nn.ModuleList()\n",
    "        for _ in range(self.edge_type_num):\n",
    "            self.adj_list.append(nn.Linear(in_features, u))\n",
    "        self.linear_2 = nn.Linear(in_features, u)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, n_tensor, adj_tensor, h_tensor=None):\n",
    "        if h_tensor is not None:\n",
    "            annotations = torch.cat((n_tensor, h_tensor), -1)\n",
    "        else:\n",
    "            annotations = n_tensor\n",
    "        adj_tensors_split = torch.unbind(adj_tensor, dim=-1)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(self.edge_type_num):\n",
    "            # Get the linear transformed node features for this edge type\n",
    "            node_features_transformed = self.adj_list[i](annotations)\n",
    "            \n",
    "            # Get the specific adjacency matrix for this edge type\n",
    "            adj = adj_tensors_split[i]\n",
    "            \n",
    "            # Perform matrix multiplication\n",
    "            # adj shape: (batch_size, num_nodes, num_nodes)\n",
    "            # node_features_transformed shape: (batch_size, num_nodes, u)\n",
    "            # matmul output shape: (batch_size, num_nodes, u)\n",
    "            outputs.append(torch.matmul(adj, node_features_transformed))\n",
    "\n",
    "        # Sum the outputs from all edge types\n",
    "        out_sum = torch.stack(outputs, dim=0).sum(dim=0)\n",
    "\n",
    "        out_linear_2 = self.linear_2(annotations)\n",
    "        output = out_sum + out_linear_2\n",
    "        output = self.activation(output) if self.activation is not None else output\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GraphAggregationLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.i = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.j = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features):\n",
    "        # i and j are neural networks\n",
    "        i_output = self.i(node_features)\n",
    "        j_output = self.j(node_features)\n",
    "        \n",
    "        aggregated_vector = torch.sum(i_output * j_output, dim=1)\n",
    "        \n",
    "        return torch.tanh(aggregated_vector)\n",
    "\n",
    "\n",
    "class MolGANDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, node_feature_dim=5, num_bond_types=4, is_reward_network=False):\n",
    "        super().__init__()\n",
    "        self.is_reward_network = is_reward_network\n",
    "\n",
    "        self.gcn_layer_1 = RelationalGraphConvLayer(node_feature_dim, 64, num_bond_types)\n",
    "        self.gcn_layer_2 = RelationalGraphConvLayer(64, 32, num_bond_types)\n",
    "\n",
    "        self.aggregation_layer = GraphAggregationLayer(32, 128)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, adjacency_tensor):\n",
    "        gcn_output_1 = self.gcn_layer_1(node_features, adjacency_tensor)\n",
    "        gcn_output_2 = self.gcn_layer_2(gcn_output_1, adjacency_tensor)\n",
    "\n",
    "        graph_representation = self.aggregation_layer(gcn_output_2)\n",
    "        \n",
    "        output = self.mlp(graph_representation)\n",
    "\n",
    "        if self.is_reward_network:\n",
    "            return torch.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a72e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "NP_model = pickle.load(gzip.open('data/NP_score.pkl.gz'))\n",
    "SA_model = {i[j]: float(i[0]) for i in pickle.load(gzip.open('data/SA_score.pkl.gz')) for j in range(1, len(i))}\n",
    "\n",
    "\n",
    "class MolecularMetrics(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def _avoid_sanitization_error(op):\n",
    "        try:\n",
    "            return op()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def remap(x, x_min, x_max):\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_lambda(x):\n",
    "        return x is not None and Chem.MolToSmiles(x) != ''\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_lambda_special(x):\n",
    "        s = Chem.MolToSmiles(x) if x is not None else ''\n",
    "        return x is not None and '*' not in s and '.' not in s and s != ''\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_scores(mols):\n",
    "        return np.array(list(map(MolecularMetrics.valid_lambda_special, mols)), dtype=np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_filter(mols):\n",
    "        return list(filter(MolecularMetrics.valid_lambda, mols))\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_total_score(mols):\n",
    "        return np.array(list(map(MolecularMetrics.valid_lambda, mols)), dtype=np.float32).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_scores(mols, data):\n",
    "        return np.array(\n",
    "            list(map(lambda x: MolecularMetrics.valid_lambda(x) and Chem.MolToSmiles(x) not in data.smiles, mols)))\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_filter(mols, data):\n",
    "        return list(filter(lambda x: MolecularMetrics.valid_lambda(x) and Chem.MolToSmiles(x) not in data.smiles, mols))\n",
    "\n",
    "    @staticmethod\n",
    "    def novel_total_score(mols, data):\n",
    "        return MolecularMetrics.novel_scores(MolecularMetrics.valid_filter(mols), data).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_scores(mols):\n",
    "        smiles = list(map(lambda x: Chem.MolToSmiles(x) if MolecularMetrics.valid_lambda(x) else '', mols))\n",
    "        return np.clip(\n",
    "            0.75 + np.array(list(map(lambda x: 1 / smiles.count(x) if x != '' else 0, smiles)), dtype=np.float32), 0, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_total_score(mols):\n",
    "        v = MolecularMetrics.valid_filter(mols)\n",
    "        s = set(map(lambda x: Chem.MolToSmiles(x), v))\n",
    "        return 0 if len(v) == 0 else len(s) / len(v)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def novel_and_unique_total_score(mols, data):\n",
    "    #     return ((MolecularMetrics.unique_scores(mols) == 1).astype(float) * MolecularMetrics.novel_scores(mols,\n",
    "    #                                                                                                       data)).sum()\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def reconstruction_scores(data, model, session, sample=False):\n",
    "    #\n",
    "    #     m0, _, _, a, x, _, f, _, _ = data.next_validation_batch()\n",
    "    #     feed_dict = {model.edges_labels: a, model.nodes_labels: x, model.node_features: f, model.training: False}\n",
    "    #\n",
    "    #     try:\n",
    "    #         feed_dict.update({model.variational: False})\n",
    "    #     except AttributeError:\n",
    "    #         pass\n",
    "    #\n",
    "    #     n, e = session.run([model.nodes_gumbel_argmax, model.edges_gumbel_argmax] if sample else [\n",
    "    #         model.nodes_argmax, model.edges_argmax], feed_dict=feed_dict)\n",
    "    #\n",
    "    #     n, e = np.argmax(n, axis=-1), np.argmax(e, axis=-1)\n",
    "    #\n",
    "    #     m1 = [data.matrices2mol(n_, e_, strict=True) for n_, e_ in zip(n, e)]\n",
    "    #\n",
    "    #     return np.mean([float(Chem.MolToSmiles(m0_) == Chem.MolToSmiles(m1_)) if m1_ is not None else 0\n",
    "    #             for m0_, m1_ in zip(m0, m1)])\n",
    "\n",
    "    @staticmethod\n",
    "    def natural_product_scores(mols, norm=False):\n",
    "\n",
    "        # calculating the score\n",
    "        scores = [sum(NP_model.get(bit, 0)\n",
    "                      for bit in Chem.rdMolDescriptors.GetMorganFingerprint(mol,\n",
    "                                                                            2).GetNonzeroElements()) / float(\n",
    "            mol.GetNumAtoms()) if mol is not None else None\n",
    "                  for mol in mols]\n",
    "\n",
    "        # preventing score explosion for exotic molecules\n",
    "        scores = list(map(lambda score: score if score is None else (\n",
    "            4 + math.log10(score - 4 + 1) if score > 4 else (\n",
    "                -4 - math.log10(-4 - score + 1) if score < -4 else score)), scores))\n",
    "\n",
    "        scores = np.array(list(map(lambda x: -4 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, -3, 1), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def quantitative_estimation_druglikeness_scores(mols, norm=False):\n",
    "        return np.array(list(map(lambda x: 0 if x is None else x, [\n",
    "            MolecularMetrics._avoid_sanitization_error(lambda: QED.qed(mol)) if mol is not None else None for mol in\n",
    "            mols])))\n",
    "\n",
    "    @staticmethod\n",
    "    def water_octanol_partition_coefficient_scores(mols, norm=False):\n",
    "        scores = [MolecularMetrics._avoid_sanitization_error(lambda: Crippen.MolLogP(mol)) if mol is not None else None\n",
    "                  for mol in mols]\n",
    "        scores = np.array(list(map(lambda x: -3 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, -2.12178879609, 6.0429063424), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_SAS(mol):\n",
    "        fp = Chem.rdMolDescriptors.GetMorganFingerprint(mol, 2)\n",
    "        fps = fp.GetNonzeroElements()\n",
    "        score1 = 0.\n",
    "        nf = 0\n",
    "        # for bitId, v in fps.items():\n",
    "        for bitId, v in fps.items():\n",
    "            nf += v\n",
    "            sfp = bitId\n",
    "            score1 += SA_model.get(sfp, -4) * v\n",
    "        score1 /= nf\n",
    "\n",
    "        # features score\n",
    "        nAtoms = mol.GetNumAtoms()\n",
    "        nChiralCenters = len(Chem.FindMolChiralCenters(\n",
    "            mol, includeUnassigned=True))\n",
    "        ri = mol.GetRingInfo()\n",
    "        nSpiro = Chem.rdMolDescriptors.CalcNumSpiroAtoms(mol)\n",
    "        nBridgeheads = Chem.rdMolDescriptors.CalcNumBridgeheadAtoms(mol)\n",
    "        nMacrocycles = 0\n",
    "        for x in ri.AtomRings():\n",
    "            if len(x) > 8:\n",
    "                nMacrocycles += 1\n",
    "\n",
    "        sizePenalty = nAtoms ** 1.005 - nAtoms\n",
    "        stereoPenalty = math.log10(nChiralCenters + 1)\n",
    "        spiroPenalty = math.log10(nSpiro + 1)\n",
    "        bridgePenalty = math.log10(nBridgeheads + 1)\n",
    "        macrocyclePenalty = 0.\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # This differs from the paper, which defines:\n",
    "        #  macrocyclePenalty = math.log10(nMacrocycles+1)\n",
    "        # This form generates better results when 2 or more macrocycles are present\n",
    "        if nMacrocycles > 0:\n",
    "            macrocyclePenalty = math.log10(2)\n",
    "\n",
    "        score2 = 0. - sizePenalty - stereoPenalty - \\\n",
    "                 spiroPenalty - bridgePenalty - macrocyclePenalty\n",
    "\n",
    "        # correction for the fingerprint density\n",
    "        # not in the original publication, added in version 1.1\n",
    "        # to make highly symmetrical molecules easier to synthetise\n",
    "        score3 = 0.\n",
    "        if nAtoms > len(fps):\n",
    "            score3 = math.log(float(nAtoms) / len(fps)) * .5\n",
    "\n",
    "        sascore = score1 + score2 + score3\n",
    "\n",
    "        # need to transform \"raw\" value into scale between 1 and 10\n",
    "        min = -4.0\n",
    "        max = 2.5\n",
    "        sascore = 11. - (sascore - min + 1) / (max - min) * 9.\n",
    "        # smooth the 10-end\n",
    "        if sascore > 8.:\n",
    "            sascore = 8. + math.log(sascore + 1. - 9.)\n",
    "        if sascore > 10.:\n",
    "            sascore = 10.0\n",
    "        elif sascore < 1.:\n",
    "            sascore = 1.0\n",
    "\n",
    "        return sascore\n",
    "\n",
    "    @staticmethod\n",
    "    def synthetic_accessibility_score_scores(mols, norm=False):\n",
    "        scores = [MolecularMetrics._compute_SAS(mol) if mol is not None else None for mol in mols]\n",
    "        scores = np.array(list(map(lambda x: 10 if x is None else x, scores)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, 5, 1.5), 0.0, 1.0) if norm else scores\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def diversity_scores(mols, data):\n",
    "        rand_mols = np.random.choice(data.data, 100)\n",
    "        fps = [Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048) for mol in rand_mols]\n",
    "\n",
    "        scores = np.array(\n",
    "            list(map(lambda x: MolecularMetrics.__compute_diversity(x, fps) if x is not None else 0, mols)))\n",
    "        scores = np.clip(MolecularMetrics.remap(scores, 0.9, 0.945), 0.0, 1.0)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def __compute_diversity(mol, fps):\n",
    "        ref_fps = Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 4, nBits=2048)\n",
    "        dist = DataStructs.BulkTanimotoSimilarity(ref_fps, fps, returnDistance=True)\n",
    "        score = np.mean(dist)\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def drugcandidate_scores(mols, data):\n",
    "\n",
    "        scores = (MolecularMetrics.constant_bump(\n",
    "            MolecularMetrics.water_octanol_partition_coefficient_scores(mols, norm=True), 0.210,\n",
    "            0.945) + MolecularMetrics.synthetic_accessibility_score_scores(mols,\n",
    "                                                                           norm=True) + MolecularMetrics.novel_scores(\n",
    "            mols, data) + (1 - MolecularMetrics.novel_scores(mols, data)) * 0.3) / 4\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def constant_bump(x, x_low, x_high, decay=0.025):\n",
    "        return np.select(condlist=[x <= x_low, x >= x_high],\n",
    "                         choicelist=[np.exp(- (x - x_low) ** 2 / decay),\n",
    "                                     np.exp(- (x - x_high) ** 2 / decay)],\n",
    "                         default=np.ones_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be878d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty( y, x):\n",
    "        weight = torch.ones(y.size())\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx ** 2, dim=1))\n",
    "        return torch.mean((dydx_l2norm - 1) ** 2)\n",
    "\n",
    "def label2onehot(labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "        out = torch.zeros(list(labels.size()) + [dim])\n",
    "        out.scatter_(len(out.size()) - 1, labels.unsqueeze(-1), 1.)\n",
    "        return out\n",
    "\n",
    "def sample_z(batch_size):\n",
    "        return np.random.normal(0, 1, size=(batch_size, 32))\n",
    "\n",
    "def postprocess(inputs, method, temperature=1.):\n",
    "        def listify(x):\n",
    "            return x if type(x) == list or type(x) == tuple else [x]\n",
    "\n",
    "        def delistify(x):\n",
    "            return x if len(x) > 1 else x[0]\n",
    "\n",
    "        if method == 'soft_gumbel':\n",
    "            softmax = [F.gumbel_softmax(e_logits.contiguous().view(-1, e_logits.size(-1))\n",
    "                                        / temperature, hard=False).view(e_logits.size())\n",
    "                       for e_logits in listify(inputs)]\n",
    "        elif method == 'hard_gumbel':\n",
    "            softmax = [F.gumbel_softmax(e_logits.contiguous().view(-1, e_logits.size(-1))\n",
    "                                        / temperature, hard=True).view(e_logits.size())\n",
    "                       for e_logits in listify(inputs)]\n",
    "        else:\n",
    "            softmax = [F.softmax(e_logits / temperature, -1)\n",
    "                       for e_logits in listify(inputs)]\n",
    "\n",
    "        return [delistify(e) for e in (softmax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fb34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = SparseMolecularDataset()\n",
    "data.load(\"qm9_5k.sparsedataset\")\n",
    "\n",
    "b_dim = data.bond_num_types\n",
    "m_dim = data.atom_num_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(self, mols):\n",
    "        rr = 1.\n",
    "        for m in ('logp,sas,qed,unique' if self.metric == 'all' else self.metric).split(','):\n",
    "\n",
    "            if m == 'np':\n",
    "                rr *= MolecularMetrics.natural_product_scores(mols, norm=True)\n",
    "            elif m == 'logp':\n",
    "                rr *= MolecularMetrics.water_octanol_partition_coefficient_scores(mols, norm=True)\n",
    "            elif m == 'sas':\n",
    "                rr *= MolecularMetrics.synthetic_accessibility_score_scores(mols, norm=True)\n",
    "            elif m == 'qed':\n",
    "                rr *= MolecularMetrics.quantitative_estimation_druglikeness_scores(mols, norm=True)\n",
    "            elif m == 'novelty':\n",
    "                rr *= MolecularMetrics.novel_scores(mols, self.data)\n",
    "            elif m == 'dc':\n",
    "                rr *= MolecularMetrics.drugcandidate_scores(mols, self.data)\n",
    "            elif m == 'unique':\n",
    "                rr *= MolecularMetrics.unique_scores(mols)\n",
    "            elif m == 'diversity':\n",
    "                rr *= MolecularMetrics.diversity_scores(mols, self.data)\n",
    "            elif m == 'validity':\n",
    "                rr *= MolecularMetrics.valid_scores(mols)\n",
    "            else:\n",
    "                raise RuntimeError('{} is not defined as a metric'.format(m))\n",
    "\n",
    "        return rr.reshape(-1, 1)\n",
    "    \n",
    "def get_reward(self, n_hat, e_hat, method):\n",
    "        (edges_hard, nodes_hard) = postprocess((e_hat, n_hat), method)\n",
    "        edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "        mols = [data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "                for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "        reward = torch.from_numpy(self.reward(mols))\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144dab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(self, y, x):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = torch.ones(y.size()).to(self.device)\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)\n",
    "\n",
    "def label2onehot(self, labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "        out = torch.zeros(list(labels.size())+[dim]).to(self.device)\n",
    "        out.scatter_(len(out.size())-1,labels.unsqueeze(-1),1.)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50886073",
   "metadata": {},
   "outputs": [],
   "source": [
    "G =  Generator()\n",
    "D = MolGANDiscriminator()\n",
    "R = MolGANDiscriminator()\n",
    "\n",
    "G_optim = torch.optim.Adam(G.parameters(),lr=1e-3)\n",
    "D_optim = torch.optim.Adam(D.parameters(),lr=1e-3)\n",
    "R_optim = torch.optim.Adam(R.parameters(),lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d6f7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    mols, _, _, a, x, _, _, _, _ = data.next_train_batch(32)\n",
    "    z = sample_z(32)\n",
    "    a = torch.from_numpy(a)          # Adjacency.\n",
    "    x = torch.from_numpy(x)         # Nodes.\n",
    "    a_tensor = label2onehot(a, b_dim)\n",
    "    x_tensor = label2onehot(x, m_dim)\n",
    "    z = torch.from_numpy(z).float()\n",
    "    \n",
    "    logits_real, features_real = D(a_tensor, None, x_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6992b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 9, 9)\n",
      "(32, 9)\n",
      "torch.Size([32, 9, 9, 5])\n",
      "torch.Size([32, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "mols, _, _, a, x, _, _, _, _ = data.next_train_batch(32)\n",
    "a= a.astype(np.int64)\n",
    "x= x.astype(np.int64)\n",
    "print(a.shape)\n",
    "print(x.shape)\n",
    "z = sample_z(32)\n",
    "a = torch.from_numpy(a).long()         # Adjacency.\n",
    "x = torch.from_numpy(x).long()        # Nodes.\n",
    "a_tensor = label2onehot(a, b_dim)\n",
    "x_tensor = label2onehot(x, m_dim)\n",
    "z = torch.from_numpy(z).float()\n",
    "print(a_tensor.size())\n",
    "print(x_tensor.size())\n",
    "edges, adj = G(z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
