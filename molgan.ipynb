{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b03e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8517a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=32, N=9, T=5, Y=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim \n",
    "        self.N = N  \n",
    "        self.T = T  \n",
    "        self.Y = Y  \n",
    "        \n",
    "        self.atom_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, N * T),\n",
    "            nn.Softmax() \n",
    "        )\n",
    "        \n",
    "        self.adj_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, N * N * Y),\n",
    "            nn.Softmax()  \n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        x = self.atom_mlp(z) \n",
    "        x = x.view(batch_size, self.N, self.T)  # batch_size × N × T\n",
    "        \n",
    "        a = self.adj_mlp(z)  # batch_size × (N*N*Y)\n",
    "        a = a.view(batch_size, self.N, self.N, self.Y)  # batch_size × N × N × Y\n",
    "        \n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb7687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels):\n",
    "        super().__init__()\n",
    "        self.num_rels = num_rels\n",
    "        self.relation_weights = nn.ModuleList([nn.Linear(in_feat, out_feat) for _ in range(num_rels)])\n",
    "        self.self_loop = nn.Linear(in_feat, out_feat)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h: batch_size × N × in_feat\n",
    "        # adj: batch_size × N × N × num_rels\n",
    "        batch_size, N, _ = h.size()\n",
    "        out_feat = self.relation_weights[0].out_features\n",
    "        new_h = torch.zeros(batch_size, N, out_feat, device=h.device)\n",
    "\n",
    "        for r in range(self.num_rels):\n",
    "            adj_r = adj[..., r]  # batch_size × N × N\n",
    "            neigh_h = torch.bmm(adj_r, h)  # batch_size × N × in_feat\n",
    "            new_h += self.relation_weights[r](neigh_h)\n",
    "\n",
    "        # Add self-loop\n",
    "        new_h += self.self_loop(h)\n",
    "\n",
    "        return F.relu(new_h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, node_dim=5, num_rels=4, hidden_dim=64, embed_dim=32, N=50):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.rgcn_layer1 = RGCNLayer(node_dim, hidden_dim, num_rels)\n",
    "        self.rgcn_layer2 = RGCNLayer(hidden_dim, embed_dim, num_rels)  # Output to 32-dim per node\n",
    "\n",
    "        # Two-layer MLP: 32 -> 128 -> 1\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()  # Outputs between -1 and 1; adjust if strict 0-1 is needed\n",
    "        )\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # x: batch_size × N × T (node features)\n",
    "        # a: batch_size × N × N × Y (multi-relational adjacency)\n",
    "        \n",
    "        # Relational GCN layers\n",
    "        h = self.rgcn_layer1(x, a)\n",
    "        h = self.rgcn_layer2(h, a)\n",
    "        \n",
    "        # Graph aggregation: mean pooling over nodes to get 32-dim vector\n",
    "        graph_embed = h.mean(dim=1)  # batch_size × 32\n",
    "        \n",
    "        # MLP to output scalar\n",
    "        out = self.mlp(graph_embed)  # batch_size × 1\n",
    "        \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
